

分析 LinkedList , ArrayList 
《大话数据结构》或者 《算法图解》| 算法导论
算法动态可视图：
https://visualgo.net/
https://visualgo.net/en
Leetcode
参考的资料一般有：权威的专业书籍、Wikipedia、一些人的博客
《趣谈网络协议》


四个维度:
1、来源及概念
2、特性
3、适合解决的问题
4、实际的应用场景

[DS-01]|为什么要学习数据结构和算法？
1）基础内功: 
CCST-BASE  数据结构和算法，跟操作系统、计算机网络   vs  Java API、开发框架
数据结构与算法作为计算机的基础知识、核心知识，都是必须要掌握的。
目标：基础架构研发工程师，写出达到开源水平的框架才是你的目标
2）SW分析与设计实现(功能与性能): 
使用 API实现业务逻辑， 到开发设计高效性能的系统架构，分析，评估优劣效率与重构： 知其然更知其所以然；
BAT、Google、Facebook，面试的时候都喜欢考算法、让人现场写代码
性能好坏起码是其中一个非常重要的评判标准：算法是不是够优化，数据存取的效率是不是够高，内存是不是够节省等，高手竞争在于核心细节；
3) 能力提升捷径：
掌握数据结构和算法，不管对于阅读框架源码，还是理解其背后的设计思想


[DS-02]|如何抓住重点，系统高效地学习数据结构与算法？
数据结构和算法- 难学成，原因你没有找到好的学习方法，没有抓住学习的重点

1）概念及其联系: 
什么是数据结构 ？什么是算法 ？
广义上讲，数据结构就是指一组数据的逻辑结构与存储结构（2维度）。算法就是操作数据的一组方法。
狭义上讲，指某些著名的数据结构和算法，比如队列、栈、堆、二分查找、动态规划等实际场景中来，经过检验和验证的。

数据结构和算法有什么关系呢 ？
数据结构和算法是相辅相成的。数据结构是为算法服务的，算法要作用在特定的数据结构之上。

需要基础 ？
数学 - 工程化代码暂不需要《算法导论》那样严格的算法证明与推理其时空复杂度 - >结论；
编程基础 + 项目经验 :  如何提高效率、如何节省存储空间 :  从实际场景出发，“是什么”，还会教你“为什么”，并且告诉你遇到同类型问题应该“怎么做”。

2） 学习的重点在什么地方？
1\梳理一下，应该先学什么，后学什么。你可以对照看看，你属于哪个阶段，然后有针对地进行学习。
2\首先要掌握一个数据结构与算法中最重要的概念——复杂度分析。（几乎占了数据结构和算法这门课的半壁江山，是数据结构和算法学习的精髓。）
数据结构和算法解决的是如何更省、更快地存储和处理数据的问题 ---> 需要一个考量效率和资源消耗的方法，这就是复杂度分析方法 ;(大篇幅给你讲透。你也一定要花大力气来啃，必须要拿下，并且要搞得非常熟练。否则，后面的数据结构和算法也很难学好。)

3\数据结构与算法的正文内容 - 知识点一览图：
总结了20个最常用的、最基础数据结构与算法，不管是应付面试还是工作需要，只要集中精力逐一攻克这20个知识点就足够：
10个数据结构：数组、链表、栈、队列、散列表、二叉树、堆、跳表、图、Trie树；
10个算法：递归、排序、二分查找、搜索、哈希算法、贪心算法、分治算法、回溯算法、动态规划、字符串匹配算法。
不要死记硬背，要学习它的“来历”“自身的特点”“适合解决的问题”以及“实际的应用场景” ---》 能在开发中灵活应用
好的思维训练的过程，所以，千万不要被动地记忆，要多辩证地思考，多问为什么，多实践；RTFSC - 主动与被动；

一些可以让你事半功倍的学习技巧 ？
1.边学边练，适度刷题
每周花1～2个小时的时间，集中把这周的三节内容涉及的数据结构和算法，全都自己写出来，用代码实现一遍。
观点是可以“适度”刷题，但一定不要浪费太多时间在刷题上。除非你要面试Google、Facebook这样的公司，它们的算法题目非常非常难，必须大量刷题，才能在短期内提
升应试正确率。否则，国内本专栏即可；
2.多问、多思考、多互动
区写下自己的疑问、思考和总结，也可以经常看看别人的留言，和他们进行互动。
3.打怪升级学习法 - 成就感 - 反馈
4.知识需要沉淀，不要想试图一下子掌握所有
学习知识的过程是反复迭代、不断沉淀的过程 - 通用的学习方法


https://blog.csdn.net/m0_37621078/article/details/103
[DS-03]|复杂度分析（上）：如何分析、统计算法的执行效率和资源消耗？
时间，空间复杂度的概念，表示及计算方法，常见得复杂度量级及相互间的关系（大小及数据规模影响结果） - ？ 
实际测试与理论的差异如何处理 ？

数据结构和算法本身解决的是“快”和“省”的问题，即如何让代码运行得更快，如何让代码更省存储空间；
复杂度分析是整个算法学习的精髓，只要掌握了它，数据结构和算法的内容基本上就掌握了一半。

1) 为什么需要复杂度分析？
代码跑一遍，通过统计、监控，就能得到算法执行的时间和占用的内存大小。为什么还要做时间、空间复杂度分析呢？- 是一种事后统计法方法，（正确性）尤其用于具体实际产品测试验证中，事后统计法的局限性。
1. 测试结果非常依赖测试环境： 测试环境中硬件的不同
2. 测试结果受数据及规模的影响很大： 如规模较小时，插入排序可能反倒会比快速排序要快！

2）时间复杂度
大O复杂度表示法
从 CPU 的角度来看，这段代码的每一行都执行着类似的操作：读数据-运算-写数据。忽略不同指令执行时间不同（次要因素），可以得到一个非常重要的规律，那就是，所有代码的执行时间T(n)与每行代码的执行次数n成正比。

其中，T(n)我们已经讲过了，它表示代码执行的时间；n表示数据规模的大小；f(n)表示每行代码执行的次数总和。因为这是一个公式，所以用f(n)来表示。公式中的O，表示代码的执行时间T(n)与f(n)表达式成正比。大O时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的变化趋势，所以，也叫作渐进时间复杂度（asymptotic time complexity），简称时间复杂度。

既然算法复杂度主要表示的是一个相对于数据量的增长速度，并不是一个精确的衡量指标，我们需要用一个“上界”和一个“下界”来涵盖复杂度这个相对增长率。数学上，对“上界”与“下界”的定义如下：
如果存在正常数c和n0，使得当N>=n0时T(N)<=c·f(N)，则记为T(N)=O(f(N))；
如果存在正常数c和n0，使得当N>=n0时T(N)>=c·f(N)，则记为T(N)=Ω(f(N))；
第一个定义的意思就是：当N超过某个值后，c·f(N)总是至少比T(N)要大。忽略常数因子，即f(N)至少与T(N)一样大。第二个定义意思就是：当N超过某个值后，c·f(N)总是最多和T(N)一样大。

当我们说T(N)=O(f(N))时，其实就是说“T(N)是在以不快于f(N)的速度增长”，类似的T(N)=Ω(f(N))即“T(N)是在以不慢于f(N)的速度增长”。不难发现，O(f(N))就是T(N)的“上界”，Ω(f(N))就是T(N)的“下界”。由于对算法进行复杂度分析时往往考虑“最坏情况”，所以我们通常计算的是O(f(N))，即“上界”，俗称“大O阶”。

使用大O阶表示代码执行时间T(N)与代码执行总次数f(N)（N表示数据规模的大小）之间的关系式为：T(N)=O(f(N))。

对算法复杂度的分析主要是看算法执行时间随数据量增长的变化趋势，可以理解为当数据规模趋近于无穷大时，算法执行需要耗费的时间增长趋势是怎样的，也即趋近于无穷大的快慢程度。

3）计算技巧
如何分析一段代码的时间复杂度？
只关注循环执行次数最多的一段代码
加法法则：
总复杂度等于量级最大的那段代码的复杂度这个规律实际上是对上面那个规律的扩展，还是取其中最大的量级。我们将这个规律抽象成公式就是：如果 T1(n)=O(f(n))，T2(n)=O(g(n))；那么 T(n)=T1(n)+T2(n)=max(O(f(n)), O(g(n))) =O(max(f(n), g(n)))。

乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积
一个循环嵌套代码，嵌套内外层代码的时间复杂度都是O(n)，所以总的时间复杂度就是O(n * n)。
我们将这个规律抽象成公式就是：
如果 T1(n)=O(f(n))，T2(n)=O(g(n))；那么 T(n)=T1(n)*T2(n)=O(f(n))*O(g(n))=O(f(n)*g(n))。

4）几种常见的时间复杂度
粗略地分为两类：多项式量级和非多项式量级。其中，非多项式量级只有两个：O(2n) 和 O(n!)。

我们把时间复杂度为非多项式量级的算法问题叫作 NP（Non-Deterministic Polynomial，非确定多项式）问题。当数据规模 n 越来越大时，非多项式量级算法的执行时间会急剧增加，求解问题的执行时间会无限增长。所以，非多项式时间复杂度的算法其实是非常低效的算法。

	1- O(1) - 常数级
代码的执行时间不随 n 的增大而增长，这样代码的时间复杂度我们都记作 O(1)。
或者说，一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是Ο(1)。

	2- O(logn)、O(nlogn) 

对数阶时间复杂度非常常见，同时也是最难分析的一种时间复杂度
i=1;
while (i <= n) {
i = i * 2;
}
等比数列: 2

如果一段代码的时间复杂度是O(logn)，我们循环执行n遍，时间复
杂度就是O(nlogn)了。而且，O(nlogn)也是一种非常常见的算法时间复杂度。比如，归并排序、快速排序的时间复杂度都是O(nlogn)。

	3- O(m+n)、O(m*n)
int cal(int m, int n) {
int sum_1 = 0;
int i = 1;
for (; i < m; ++i) {
sum_1 = sum_1 + i;
}
int sum_2 = 0;
int j = 1;
for (; j < n; ++j) {
sum_2 = sum_2 + j;
}
return

m和n是表示两个数据规模。我们无法事先评估m和n谁的量级大，上面代码的时间复杂度就是O(m+n)。

4）空间复杂度
时间复杂度的全称是渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系。类比一下，空间复杂度全称就是渐进空间复杂
度（asymptotic space complexity），表示算法的存储空间与数据规模之间的增长关系。
常见的空间复杂度就是O(1)、O(n)、O(n2 )，像O(logn)、O(nlogn)这样的对数阶复杂度平时都用不到。
[DS-04]||复杂度分析（下）：浅析最好、最坏、平均、均摊时间复杂度
四个复杂度分析方面的知识点，
最好情况时间复杂度（best case time complexity）、
最坏情况时间复杂度（worst case time complexity）、
平均情况时间复杂度（average case time complexity）、
均摊时间复杂度（amortized time omplexity)

1、最好、最坏情况时间复杂度
查找一个数据：
int find(int[] array, int n, int x) {
int i = 0;
int pos = -1;
for (; i < n; ++i) {
if (array[i] == x) pos = i;
}
return pos;
}

优化
// n表示数组array的长度
int find(int[] array, int n, int x) {
int i = 0;
int pos = -1;
for (; i < n; ++i) {
if (array[i] == x) {
pos = i;
break;
}
}
return pos;
}

优化完之后，这段代码的时间复杂度还是O(n)吗？
最好情况时间复杂度就是，在最理想的情况下，执行这段代码的时间复杂度。- O(1)
最坏情况时间复杂度就是，在最糟糕的情况下，执行这段代码的时间复杂度。- O(n)

2、平均情况时间复杂度：（概率统计在内）平均时间复杂度的全称应该叫加权平均时间复杂度或者期望时间复杂度。

时间复杂度的大O标记法中，可以省略掉系数、低阶、常量 - O(n)
同一块代码在不同的情况下，时间复杂度有量级的差距，我们才会使用这三种复杂度表示法来区分。

3、均摊时间复杂度，以及它对应的分析方法，摊还分析（或者叫平摊分析）。
均摊时间复杂度应用的场景比它更加特殊、更加有限。
// array表示一个长度为n的数组
// 代码中的array.length就等于n
int[] array = new int[n];
int count = 0;
void insert(int val) {
if (count == array.length) {
int sum = 0;
for (int i = 0; i < array.length; ++i) {
sum = sum + array[i];
}
array[0] = sum;
count = 1;
}
array[count] = val;
++count;
}

用 三种时间复杂度的分析方法来分析：
最好情况： 时间复杂度为O(1)
最坏情况：数组中没有空闲空间了，我们需要先做一次数组的遍历求和，然后再将数据插入，所以最坏情况时间复杂度为O(n)。
那平均时间复杂度是多少呢？答案是O(1) - 也可以理解为概率相同的均摊


但是这个例子里的平均复杂度分析其实并不需要这么复杂： 
首先，find()函数在极端情况下，复杂度才为O(1)。但insert()在大部分情况下，时间复杂度都为O(1)。只有个别情况下，复杂度才比较高，为O(n)
第二
对于insert()函数来说，O(1)时间复杂度的插入和O(n)时间复杂度的插入，出现的频率是非常有规律的，而且有一定的前后时序关系，一般都是一个O(n)插入之后，紧跟着n-1个O(1)的插入操作，循环往复。

针对这样一种特殊场景的复杂度分析，我们并不需要像之前讲平均复杂度分析方法那样，找出所有的输入情况及相应的发生概率，然后再计算加权平均
值。
针对这种特殊的场景，我们引入了一种更加简单的分析方法：摊还分析法，通过摊还分析得到的时间复杂度我们起了一个名字，叫均摊时间复杂度。
每一次O(n)的插入操作，都会跟着n-1次O(1)的插入操作，所以把耗时多的那次操作均摊到接下来的n-1次耗时少的
操作上，均摊下来，这一组连续的操作的均摊时间复杂度就是O(1)。这就是均摊分析的大致思路。
对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系，这个
时候，我们就可以将这一组操作放在一块儿分析，看是否能将较高时间复杂度那次操作的耗时，平摊到其他那些时间复杂度比较低的操作上。
均摊时间复杂度分析的场合，一般均摊时间复杂度就等于最好情况时间复杂度。


你可以用今天学习的知识，来分析一下下面这个add()函数的时间复杂度。
// 全局变量，大小为10的数组array，长度len，下标i。
int array[] = new int[10];
int len = 10;
int i = 0;
// 往数组中添加一个元素
void add(int element) {
if (i >= len) { // 数组空间不够了
// 重新申请一个2倍大小的数组空间
int new_array[] = new int[len*2];
// 把原来array数组中的数据依次copy到new_array
for (int j = 0; j < len; ++j) {
new_array[j] = array[j];
}
// new_array复制给array，array现在大小就是2倍len了
array = new_array;
len = 2 * len;
}
// 将element放到下标为i的位置，下标i加一
array[i] = element;
++
}





[DS-05]|数组：为什么很多编程语言中数组都从0开始编号？

1、概念
数组：数组（Array）是一种线性表数据结构。它用一组连续的内存空间，来存储一组具有相同类型的数据。
关键词：
第一是线性表（Linear List）：每个线性表上的数据最多只有前和后两个方向（任意元素间关系:至多有一个前驱与后继元素）: 其实除了数组，链表、队列、栈等也是线性表结构。
数据间的关系 - 线性结结构与非线性结构（ 而与它相对立的概念是非线性表，比如二叉树、堆、图等。之所以叫非线性，是因为，在非线性表中，数据之间并不是简单的前后关系。）
第二个是连续的内存空间和相同类型的数据。

2、特点： 随机访问
由于1，2 两个关键点，可以推出数组重要特性：随机访问（数组下标）

当计算机需要随机访问数组中的某个元素时，它会首先通过下面的寻址公式，计算出该元素存储的内存地址：a[i]_address = base_address + i * data_type_size
1）下标访问数据元素复杂度 O(1)
2）插入、删除复杂度较高 - 平均 O(n)
插入:
有序数组插入某个元素到位置 K ，需要腾挪后面数组元素；
无序数组插入某个元素到位置 K,  则可避免腾挪数组元素，只需要第 K 元素放入表尾，新元素放 K 位置，复杂度 O(1) - 快排中也会用.
删除:
一般情况下，删除数组首元素时间复杂度 O(n), 尾元素 O(1),  平均 O(n);
特殊情况下，如存在批量删除数组元素情况，可采用标记-延迟删除方法，避免单次删除1个元素导致频繁移动。先标记待删除数组元素，等数组空间不够用时，再触发实际删除标记元素操作；

如果你了解JVM，你会发现，这不就是JVM标记清除垃圾回收算法的核心思想吗？并不是要去死记硬背某个数据结构或者算法，而是要学习它背后的思想和处理技巧 - 额外的收益；软件开发还是架构设计中，总能找到某些算法和数据结构的影子。
3）注意事项: 警惕数组的访问越界问题
C 语言未定义数组越界的行为，只要非受限内存，则可正常访问；- 漏洞攻击行为；
Java本身就会做越界检查 - 会抛出java.lang.ArrayIndexOutOfBoundsException；

3、解决什么问题
4、实用场景
容器能否完全替代数组 ？容器类，比如Java中的ArrayList、C++ STL中的vector。
项目开发中，什么时候适合用数组，什么时候适合用容器呢？
ArrayList最大的优势就是可以将很多数组操作的细节封装起来，并支持动态扩容。
如果事先能确定需要存储的数据大小，最好在创建ArrayList的时候事先指定数据大小。- 避免扩容及数据搬迁；
数组与容器类何时实用 ？
1）.Java ArrayList无法存储基本类型，比如int、long，需要封装为Integer、Long类，而开箱与拆箱Autoboxing、Unboxing则有一定的性能消耗，所以如果特别关注性能，或者希望使用基本类型，就可以选用数组。
2）、如果数据大小事先已知，并且对数据的操作非常简单，用不到ArrayList提供的大部分方法，也可以直接使用数组。
3）、当要表示多维数组时，用数组往往会更加直观。比如Object[][] array；而用容器的话则需要这样定义：ArrayList<ArrayList > array。
4）、做一些非常底层的开发，比如开发网络框架，性能的优化需要做到极致，这个时候数组就会优于容器，成为首选。

5、解答： 什么很多编程语言中数组都从0开始编号？
从数组存储的内存模型上来看，“下标”最确切的定义应该是“偏移（offset）”
从 0 开始: 计算公式
a[k]_address = base_address + k * type_size
从 1 开始: 计算公式
a[k]_address = base_address + (k-1)*type_size
从1开始编号，每次随机访问数组元素都多了一次减法运算，对于CPU来说，就是多了一次减法指令。

数组作为非常基础的数据结构，通过下标随机访问数组元素又是其非常基础的编程操作，效率的优化就要尽可能做到极致。所以为了减少一次减法操作，数组选择了从0开始编号，而不是从1开始。

最主要的原因可能是历史原因： C语言设计者用0开始计数数组下标，之后的Java、JavaScript等高级语言都效仿了C语言，或者说，为了在一定程度上减少C语言程序员学习Java的学习成本；实际上，很多语言中数组也并不是从0开始计数的，比如Matlab。甚至还有一些语言支持负数下标，比如Python。

6、课后题
1）JVM 垃圾标记清除算法
2）二维数组的内存计算公式
1. 不同的语言对数组访问越界的处理方式不同，即便是同一种语言，不同的编译器处理的方式也不同。至于你熟悉的语言是怎么处理的，请行百度。
2. C语言中，数组访问越界的处理是未决。并不一定是错，有同学做实验说没问题，那并不代表就是正确的。
3. 我觉得那个例子，栈是由高到低位增长的，所以，i和数组的数据从高位地址到低位地址依次是：i, a[2], a[1], a[0]。a[3]通过寻址公式，计算得到地址正好
是i的存储地址，所以a[3]=0，就相当于i=0.
4. 大家有不懂的多看看留言，留言区还是有很多大牛的！我可能有时候回复的不及时，或者同样的问题只回复一个同学！

[DS-06]|链表（上）：如何实现LRU缓存淘汰算法
https://www.ibm.com/developerworks/cn/linux/l-overflow/
https://www.ibm.com/developerworks/cn/linux/l-cn-gccstack/index.html
1、链表
单链表，循环链表，双链表 =》 双向循环链表 ： 各种链表的特点及应用中的优缺点 - 迭代更新
插入，删除，遍历操作及时间复杂度分析，对比 
空间换时间的思想
应用场景中数据结果选择： 数组 vs 链表
Java LinkedList 分析
2、LRU cache 淘汰算法实践
3、练习T

[DS-07]|链表（下）：如何轻松写出正确的链表代码？上

写好链表相关代码:  时间 + 精力 + 练习 + 方法与技巧（注意点） ==> 熟练
6 个技巧:
技巧一：理解指针或引用的含义
技巧二：警惕指针丢失和内存泄漏
技巧三：利用哨兵简化实现难度
技巧四：重点留意边界条件处理
技巧五：举例画图，辅助思考
技巧六：多写多练，没有捷径


[DS-08]|栈：如何实现浏览器的前进和后退功能？

1、概念及实现
来源: 盘叠
定义: 受限线性表结果-单端进出、先进后出
实现:  主要接口(push , pop , peek, isEmpty)，顺序，链式栈，支持动态扩容的栈(动态扩容)，时空复杂度(均摊)；
2、特性：先进后出，后进先出
3、适用解决问题: 与特性相关
4、实际应用场景
4.1、表达式求值；
4.2、括号匹配；
4.3、chrome 浏览器 label -》 跳转 实现: a  -> b -> c ;  c ->b ->d  ? 

内存中的堆栈和数据结构堆栈不是一个概念，可以说内存中的堆栈是真实存在的物理区，数据结构中的堆栈是抽象的数据存储结构。
内存空间在逻辑上分为三部分：代码区、静态数据区和动态数据区，动态数据区又分为栈区和堆区。
代码区：存储方法体的二进制代码。高级调度（作业调度）、中级调度（内存调度）、低级调度（进程调度）控制代码区执行代码的切换。

静态数据区：存储全局变量、静态变量、常量，常量包括final修饰的常量和String常量。系统自动分配和回收。
栈区：存储运行方法的形参、局部变量、返回值。由系统自动分配和回收。
堆区：new一个对象的引用或地址存储在栈区，指向该对象存储在堆区中的真实数据。

eetcode上关于栈的题目大家可以先做20,155,232,844,224,682,496.

为什么函数调用要用“栈”来保存临时变量呢？用其他数据结构不行吗？
其实，我们不一定非要用栈来保存临时变量，只不过如果这个函数调用符合后进先出的特性，用栈这种数据结构来实现，是最顺理成章的选择。
从调用函数进入被调用函数，对于数据来说，变化的是什么呢？是作用域。所以根本上，只要能保证每进入一个新的函数，都是一个新的作用域就可以。而
要实现这个，用栈就非常方便。在进入被调用函数的时候，分配一段栈空间给这个函数的变量，在函数结束的时候，将栈顶复位，正好回到调用函数的作用
域内。

JVM中的“栈”应该有两个。
一个是每个线程中方法调用用到的栈。该栈以栈帧为元素，当调用一个方法时，会把方法相关的局部变量表、操作数栈、方法返回地址等信息封装到栈帧中，把该栈帧入栈；当方法执行结束后，把该栈帧出栈。

第二个栈就是栈帧中的操作数栈。JVM的解释执行引擎是“基于栈的执行引擎”，是因为JVM的指令都是对操作数栈中的元素进行入栈出栈操作。两者应该都是标准的栈。 [6赞]


[DS-09]队列：队列在线程池等有限资源池中的应用

1、概念及实现
来源: 排队狗牌哦
定义: 受限线性表结构-队首 - 入队，队尾 - 出队
实现:  主要接口(enqueue , dequeue)
顺序队列，操作及时间复杂度分析；
链式队列，，主要操作及时空复杂度分析；
问题： 顺序队列-队满空间浪费情况，-》 队满有空间情况下，入队受阻时统一腾挪空间；操作实现及复杂度分析（均摊 ？）；--》 循环队列: 再优化不用移动 --> 设计及实现：判断队列空与满的条件；
2、特性：先进先出
3、适用解决问题: 与特性相关
4、实际应用场景
队列的应用也非常广泛，特别是一些具有某些额外特性的队列，比如循环队列、
阻塞队列、并发队列。它们在很多偏底层系统、框架、中间件的开发中，起着关键性的作用。比如高性能队列Disruptor、Linux环形缓存，都用到了循环并发队
列；Java concurrent并发包利用ArrayBlockingQueue来实现公平锁等
4.1、阻塞队列 
队列 + 阻塞操作： 生产 - 消费者模型；调整同步与协调速度-多消费者；
4.2、并发队列（队列操作线程安全队列）
队列 + 锁 （基于数组的循环队列，利用CAS原子操作 - 可以实现非常高效的并发队列 如 Disruptor）
4.3、 应用在任何有限资源池中，用于排队请求（如数据库连接资源有限的场景等）

5、引论问题回答： 
线程池没有空闲线程时，新的任务请求线程资源时，线程池该如何处理？又如何实现 ？
非阻塞 - 拒绝请求；
阻塞 - 请求排队 -》 实现存储排队： 
	- 基于链表的无界队列（unbounded queue）:  缺点队列过长，响应时间过长 - 不适合响应时间敏感系统；
	- 基于数组的有界队列（bounded queue）： 缺点-队列过短则后续请求被拒绝，- 适合响应时间敏感系统；- 合理的队列大小很重要；高效与发挥系统资源利用率；

6、问题：
6.1、有限资源： 池结构 + 请求队列；
6.2、并发队列： 无锁并发队列 ？

[DS-10]|递归：如何用三行代码找到“最终推荐人”？

数据结构和算法 - 最难： 两个最难理解的知识点，一个是动态规划，另一个就是递归
1、概念及实现
1.1、理解递归: 
引用例子: 计算在第几排 ?  f(n) = f(n-1) + 1 其中，f(1)=1
标准的递归求解问题的分解过程，去的过程叫“递”，回来的过程叫“归”;
基本上，所有的递归问题都可以用递推公式来表示 ; 
1.2、递归需要满足的三个条件：
1）分解子问题：规模
2）问题相似：除规模和求解思路外其他都相似
3）存在递归终止条件
1.3、如何编写递归代码？
写递归代码最关键的是写出递推公式，找到终止条件；
将递推公式转化为代码实现；
注意点: 
1） 避免人肉递归:  对于递归代码，这种试图想清楚整个递和归过程的做法，实际上是进入了一个思维误区。
正确做法:  只需要思考问题A与子问题B、C、D两层之间的关系即可，其他交给递推；
类似数学归纳法；
2）递归代码要警惕堆栈溢出
原因: 函数调用涉及临时变量入栈，系统栈 and JVM 栈小，递归深度大，可能导致溢出；- 堆栈溢出会造成系统性崩溃
规避方法: 避免递归深度过大，静态检查递归深度或者动态计算栈可用大小与递归深度关系(复杂)；转换非递归；递归+用户分配栈（自维护舍弃系统栈）
3）递归代码要警惕重复计算 - 缓存计算结果
解空间较大 ： 指数级不可取 -》 多项式级别；
4）递归的时间/空间 - 效率
递归-函数入栈/出栈：时间，空间复杂度跟递归深度(规模有关)
1.4、怎么将递归代码改写为非递归代码？
递归有利有弊，利是递归代码的表达力很强，写起来非常简洁；
而弊就是空间复杂度高、有堆栈溢出的风险、存在重复计算、过多的函数调用会耗时较多；
1）所有的递归代码都可以改为迭代循环的非递归写法；
2）借鉴实现原理: 手动建栈 + 模拟入栈/出栈实现；- 增加了实现复杂度；

2、特性
3、适用问题
4、实用场景
1）例子：假如这里有n个台阶，每次你可以跨1个台阶或者2个台阶，请问走这n个台阶有多少种走法？
从结束点出发，倒推 :
递推公式: f(n) = f(n-1) + f(n-2)
终止条件: f(1) = 1 , f(2) = ? 2 探寻合适的终止条件| 拿 f(3)  /f（4） 验证终止条件结果是否正确 
2）
5、思考：
1）递归代码，你有什么好的调试方法呢？
调试递归:
1.打印日志发现，递归值。
2.结合条件断点进行调试。
2）检测环可以构造一个set集合或者散列表 











4要素入手: 来历，特点，用途，实用场景(经验总结)
王铮 algo github 资源（多PL：c/c++/java/python/go/swift/php/js/）: https://github.com/wangzheng0822/algo
{ 微信搜索我的公众号“小争哥”，或者微信扫描下面二维码关注
关注微信公众号，回复”PDF“获取独家算法资料。
前Google工程师，10万人跟着学的《数据结构和算法之美》《设计模式之美》专栏作者 }
Java 自动装箱/拆箱操作 - 》 参考连接: https://www.cnblogs.com/wang-yaz/p/8516151.html
我们今天学习了数组。它可以说是最基础、最简单的数据结构了。数组用一块连续的内存空间，来存储相同类型的一组数据，最大的特点就是支持随机访问，但
插入、删除操作也因此变得比较低效，平均情况时间复杂度为O(n)。在平时的业务开发中，我们可以直接使用编程语言提供的容器类，但是，如果是特别底层的
开发，直接使用数组可能会更合适。


